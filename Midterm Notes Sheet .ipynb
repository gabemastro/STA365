{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0066468f",
   "metadata": {},
   "source": [
    "**Bayesian Statistics Midterm Reference Guide**\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Metropolis-Hastings Algorithm (20%)**\n",
    "**Relevant Notebook:** Bayes_03_Metropolis_Hastings.ipynb (Q3)\n",
    "\n",
    "### **Key Concepts:**\n",
    "- **Goal:** Sample from a posterior distribution using a proposal distribution.\n",
    "- **Steps:**\n",
    "  1. Choose a **proposal distribution** $q(x' | x)$.\n",
    "  2. Compute **acceptance probability**:\n",
    "     $[ A = \\min \\left( 1, \\frac{p(x') q(x | x')}{p(x) q(x' | x)} \\right) ]$\n",
    "  3. Accept \\( x' \\) with probability \\( A \\), else retain \\( x \\).\n",
    "\n",
    "### **How Problems Were Solved in Homework:**\n",
    "- Used a mixture of normal and uniform proposals for efficiency.\n",
    "- Adjusted proposal variance to balance acceptance rate and exploration.\n",
    "- Ensured proper initialization to avoid early rejections.\n",
    "\n",
    "### **Code Snippet:**\n",
    "```python\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "def metropolis_hastings(target_pdf, proposal_sampler, n_samples=1000, x_init=0):\n",
    "    samples = [x_init]\n",
    "    for _ in range(n_samples):\n",
    "        x_new = proposal_sampler(samples[-1])\n",
    "        accept_ratio = target_pdf(x_new) / target_pdf(samples[-1])\n",
    "        if np.random.rand() < accept_ratio:\n",
    "            samples.append(x_new)\n",
    "        else:\n",
    "            samples.append(samples[-1])\n",
    "    return np.array(samples)\n",
    "```\n",
    "\n",
    "### **Potential Variations & How to Address Them:**\n",
    "- **Different Proposal Distributions:**\n",
    "  ```python\n",
    "  def laplace_proposal(x):\n",
    "      return np.random.laplace(x, scale=1.0)\n",
    "  ```\n",
    "- **Multivariate MH:**\n",
    "  ```python\n",
    "  def multivariate_proposal(x, cov_matrix):\n",
    "      return np.random.multivariate_normal(x, cov_matrix)\n",
    "  ```\n",
    "- **Adaptive Metropolis:**\n",
    "  ```python\n",
    "  def adaptive_proposal(x, step_size):\n",
    "      return np.random.normal(x, step_size)\n",
    "  ```\n",
    "\n",
    "### **Common Mistakes & Debugging Fixes:**\n",
    "- **Low acceptance rate:** Tune step size dynamically:\n",
    "  ```python\n",
    "  step_size = 0.1\n",
    "  if accept_ratio < 0.2:\n",
    "      step_size *= 0.9\n",
    "  elif accept_ratio > 0.8:\n",
    "      step_size *= 1.1\n",
    "  ```\n",
    "- **Slow mixing:** Increase proposal range:\n",
    "  ```python\n",
    "  proposal_sampler = lambda x: np.random.normal(x, scale=2.0)\n",
    "  ```\n",
    "- **Local mode trapping:** Implement mixture proposals:\n",
    "  ```python\n",
    "  proposal_sampler = lambda x: np.random.choice([np.random.normal(x, 0.5), np.random.normal(x, 2.0)])\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "## **2. PyMC for Bayesian Inference (40%)**\n",
    "**Relevant Notebooks:** Bayes_04_PyMC_Universal_Samplers.ipynb (Q2)\n",
    "\n",
    "### **Key Concepts:**\n",
    "- **PyMC Model Specification:** Define priors, likelihoods, and posterior sampling.\n",
    "- **NUTS Sampler:** No-U-Turn Sampler (automatically selects step size).\n",
    "\n",
    "### **How Problems Were Solved in Homework:**\n",
    "- Defined hierarchical models and assessed sensitivity to priors.\n",
    "- Diagnosed sampling efficiency using ArviZ diagnostics.\n",
    "- Ensured proper chain mixing by running multiple chains.\n",
    "\n",
    "### **Code Snippet (Fixing Common Issues):**\n",
    "- **Increasing `target_accept` to prevent divergences:**\n",
    "  ```python\n",
    "  with model:\n",
    "      trace = pm.sample(1000, target_accept=0.9, return_inferencedata=True)\n",
    "  ```\n",
    "- **Handling bad priors with prior predictive checks:**\n",
    "  ```python\n",
    "  with model:\n",
    "      prior_checks = pm.sample_prior_predictive()\n",
    "  az.plot_ppc(prior_checks)\n",
    "  ```\n",
    "- **Ensuring good convergence by increasing chains and tuning:**\n",
    "  ```python\n",
    "  with model:\n",
    "      trace = pm.sample(2000, chains=4, tune=1000, return_inferencedata=True)\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Gibbs Sampling (40%)**\n",
    "**Relevant Notebook:** Bayes_02_The_Normal-Gamma_Model.ipynb (Q1, Q2)\n",
    "\n",
    "### **Key Concepts:**\n",
    "- **Iteratively sample each variable** conditioned on the others.\n",
    "- **Metropolis-within-Gibbs:** Combines Metropolis-Hastings for non-conjugate updates.\n",
    "\n",
    "### **How Problems Were Solved in Homework:**\n",
    "- Used Normal-Gamma conjugacy for efficient sampling.\n",
    "- Applied Metropolis steps for discrete parameters within Gibbs.\n",
    "- Monitored chain convergence using trace plots.\n",
    "\n",
    "### **Code Snippet (Fixing Common Issues):**\n",
    "- **Fixing slow mixing with better initialization:**\n",
    "  ```python\n",
    "  mu = np.mean(x_data) + np.random.normal(0, 1)\n",
    "  sigma2 = np.var(x_data) * np.random.gamma(2, 2)\n",
    "  ```\n",
    "- **Handling numerical instability using log-sampling:**\n",
    "  ```python\n",
    "  log_sigma2 = np.log(np.random.gamma(alpha_n, 1/beta_n))\n",
    "  sigma2 = np.exp(log_sigma2)\n",
    "  ```\n",
    "- **Improving convergence using multiple chains:**\n",
    "  ```python\n",
    "  samples1 = gibbs_sampler(1000, x_data)\n",
    "  samples2 = gibbs_sampler(1000, x_data)\n",
    "  plt.plot(samples1[:, 0], label='Chain 1')\n",
    "  plt.plot(samples2[:, 0], label='Chain 2')\n",
    "  plt.legend()\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "## **4. General Debugging Tips:**\n",
    "- **Autocorrelation Check:** High autocorrelation indicates poor mixing.\n",
    "  ```python\n",
    "  az.plot_autocorr(trace, var_names=[\"alpha\", \"beta\"])\n",
    "  ```\n",
    "- **Burn-in Period:** Discard initial samples to ensure convergence.\n",
    "  ```python\n",
    "  trace_posterior = trace.sel(draw=slice(500, None))\n",
    "  ```\n",
    "- **Posterior Predictive Checks:** Use `az.plot_ppc(trace)` to validate model assumptions.\n",
    "  ```python\n",
    "  az.plot_ppc(az.from_pymc3(trace))\n",
    "  ```\n",
    "- **Alternative Sampling Methods:** If Gibbs is slow, try Hamiltonian Monte Carlo.\n",
    "  ```python\n",
    "  with model:\n",
    "      trace = pm.sample(1000, step=pm.NUTS(), return_inferencedata=True)\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "This **expanded reference guide** now includes **code snippets for debugging solutions**, ensuring you have direct implementations for any potential midterm modifications!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9203dfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
